{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0734acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoGluon - Comparación de modelos con distintos LAGs para predicción febrero 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Importar librerías\n",
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b04002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Cargar datasets\n",
    "df_sellin = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df_productos = pd.read_csv(\"tb_productos.txt\", sep=\"\\t\")\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    product_ids = [int(line.strip()) for line in f if line.strip().isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Preprocesamiento\n",
    "df_sellin['timestamp'] = pd.to_datetime(df_sellin['periodo'], format='%Y%m')\n",
    "df_filtered = df_sellin[\n",
    "    (df_sellin['timestamp'] <= '2019-12-01') &\n",
    "    (df_sellin['product_id'].isin(product_ids))\n",
    "]\n",
    "df_grouped = df_filtered.groupby(['timestamp', 'customer_id', 'product_id'], as_index=False)['tn'].sum()\n",
    "df_monthly_product = df_grouped.groupby(['timestamp', 'product_id'], as_index=False)['tn'].sum()\n",
    "df_monthly_product['item_id'] = df_monthly_product['product_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15173341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agregar lags manualmente ---\n",
    "df_lags = df_monthly_product.copy()\n",
    "df_lags = df_lags.sort_values(by=['product_id', 'timestamp'])\n",
    "\n",
    "# Agregar 6 lags\n",
    "for i in range(1, 7):\n",
    "    df_lags[f'tn_lag_{i}'] = df_lags.groupby('product_id')['tn'].shift(i)\n",
    "\n",
    "# Eliminar filas con NaNs (por ejemplo, las primeras 6 de cada producto)\n",
    "df_lags = df_lags.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4f1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename 'tn' to 'target' as required by AutoGluon\n",
    "\n",
    "\n",
    "\n",
    "df_lags_renamed = df_lags.rename(columns={'tn': 'target'})\n",
    "ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    df_lags_renamed,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "ts_data = ts_data.fill_missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6cb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lags_renamed['item_id'] = df_lags['product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5afea4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"modelo_con_lags_manual/\"\n",
      "Beginning AutoGluon training...\n",
      "AutoGluon will save models to '/Users/patricialorenasarmientotagle/austral-labo-iii/notebooks/modelo_con_lags_manual'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       8.59 GB / 24.00 GB (35.8%)\n",
      "Disk Space Avail:   275.54 GB / 460.43 GB (59.8%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 1,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'verbosity': 2}\n",
      "\n",
      "train_data with frequency 'IRREG' has been resampled to frequency 'MS'.\n",
      "Provided train_data has 17719 rows (NaN fraction=0.1%), 734 time series. Median time series length is 30 (min=1, max=30). \n",
      "\tRemoving 78 short time series from train_data. Only series with length >= 6 will be used for training.\n",
      "\tAfter filtering, train_data has 17507 rows (NaN fraction=0.1%), 656 time series. Median time series length is 30 (min=8, max=30). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['product_id', 'tn_lag_1', 'tn_lag_2', 'tn_lag_3', 'tn_lag_4', 'tn_lag_5', ...]\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-06-09 20:32:32\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. \n",
      "\t-0.2535       = Validation score (-WQL)\n",
      "\t0.02    s     = Training runtime\n",
      "\t1.81    s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. \n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.2597       = Validation score (-WQL)\n",
      "\t0.68    s     = Training runtime\n",
      "\t0.02    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. \n",
      "\t-0.1994       = Validation score (-WQL)\n",
      "\t7.53    s     = Training runtime\n",
      "\t0.05    s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. \n",
      "\t-0.2899       = Validation score (-WQL)\n",
      "\t0.02    s     = Training runtime\n",
      "\t0.14    s     = Validation (prediction) runtime\n",
      "Training timeseries model DynamicOptimizedTheta. \n",
      "\t-0.2382       = Validation score (-WQL)\n",
      "\t0.02    s     = Training runtime\n",
      "\t0.76    s     = Validation (prediction) runtime\n",
      "Training timeseries model AutoETS. \n",
      "\t-0.2252       = Validation score (-WQL)\n",
      "\t0.01    s     = Training runtime\n",
      "\t0.89    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. \n",
      "\t-0.2263       = Validation score (-WQL)\n",
      "\t1.51    s     = Training runtime\n",
      "\t3.13    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. \n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tFine-tuning on the CPU detected. We recommend using a GPU for faster fine-tuning of Chronos.\n",
      "\tSaving fine-tuned model to /Users/patricialorenasarmientotagle/austral-labo-iii/notebooks/modelo_con_lags_manual/models/ChronosFineTuned[bolt_small]/W0/fine-tuned-ckpt\n",
      "\t-0.2001       = Validation score (-WQL)\n",
      "\t133.29  s     = Training runtime\n",
      "\t0.34    s     = Validation (prediction) runtime\n",
      "Training timeseries model TemporalFusionTransformer. \n",
      "\t-0.1846       = Validation score (-WQL)\n",
      "\t63.86   s     = Training runtime\n",
      "\t0.22    s     = Validation (prediction) runtime\n",
      "Training timeseries model DeepAR. \n",
      "\t-0.1948       = Validation score (-WQL)\n",
      "\t37.87   s     = Training runtime\n",
      "\t0.15    s     = Validation (prediction) runtime\n",
      "Training timeseries model PatchTST. \n",
      "\t-0.1842       = Validation score (-WQL)\n",
      "\t58.21   s     = Training runtime\n",
      "\t0.09    s     = Validation (prediction) runtime\n",
      "Training timeseries model TiDE. \n",
      "\t-0.1880       = Validation score (-WQL)\n",
      "\t41.48   s     = Training runtime\n",
      "\t0.21    s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoETS': 0.05, 'DeepAR': 0.23, 'PatchTST': 0.04, 'TemporalFusionTransformer': 0.31, 'TiDE': 0.36}\n",
      "\t-0.1759       = Validation score (-WQL)\n",
      "\t0.46    s     = Training runtime\n",
      "\t1.57    s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE', 'WeightedEnsemble']\n",
      "Total runtime: 353.25 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.1759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.timeseries.predictor.TimeSeriesPredictor at 0x1193dd910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ts_data_lags = TimeSeriesDataFrame.from_data_frame(\n",
    "    df_lags_renamed,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "ts_data_lags = ts_data_lags.fill_missing_values()\n",
    "\n",
    "predictor_lags = TimeSeriesPredictor(\n",
    "    prediction_length=1,\n",
    "    target='target',\n",
    "    freq='MS',\n",
    "    path='modelo_con_lags_manual/'\n",
    ")\n",
    "\n",
    "predictor_lags.fit(ts_data_lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa6f53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data with frequency 'IRREG' has been resampled to frequency 'MS'.\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n",
      "\tWarning: AutoETS/W0 failed for 78 time series (10.6%). Fallback model SeasonalNaive was used for these time series.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>1312.212270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>1066.463859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>741.460424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>558.979393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>507.088888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id           tn\n",
       "0       20001  1312.212270\n",
       "1       20002  1066.463859\n",
       "2       20003   741.460424\n",
       "3       20004   558.979393\n",
       "4       20005   507.088888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Predecir febrero 2020 ---\n",
    "forecast = predictor_lags.predict(ts_data_lags)\n",
    "\n",
    "# --- Extraer predicción media por producto ---\n",
    "forecast_feb2020 = forecast['mean'].reset_index()[['item_id', 'mean']]\n",
    "forecast_feb2020.columns = ['product_id', 'tn']\n",
    "\n",
    "# --- Exportar resultados a CSV ---\n",
    "forecast_feb2020.to_csv(\"predicciones_febrero2020_lags_manual.csv\", index=False)\n",
    "\n",
    "# --- Mostrar primeros resultados ---\n",
    "forecast_feb2020.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be89f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicciones = pd.read_csv(\"predicciones_febrero2020_lags_manual.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de652992",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    product_ids = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "df_objetivo = pd.DataFrame({\"product_id\": product_ids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36929ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Total de productos faltantes: 46\n",
      "Algunos ejemplos: [20127, 20210, 20213, 20236, 20257, 20261, 20440, 20615, 20621, 20662, 20673, 20686, 20694, 20703, 20711, 20720, 20762, 20815, 20845, 20886, 20899, 20904, 20910, 20912, 20933, 20942, 20953, 20962, 20968, 20975, 20995, 21006, 21035, 21039, 21058, 21079, 21087, 21097, 21109, 21110, 21112, 21129, 21144, 21146, 21154, 21214]\n"
     ]
    }
   ],
   "source": [
    "# Hacer merge para ver cuáles faltan\n",
    "df_merged = df_objetivo.merge(df_predicciones, on=\"product_id\", how=\"left\", indicator=True)\n",
    "\n",
    "# Filtrar los que no fueron predichos\n",
    "faltantes = df_merged[df_merged[\"_merge\"] == \"left_only\"][\"product_id\"].tolist()\n",
    "\n",
    "print(f\"❗ Total de productos faltantes: {len(faltantes)}\")\n",
    "print(\"Algunos ejemplos:\", faltantes[:46])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e32ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Faltan predicciones para 46 productos.\n",
      "✅ Calculados promedios para 46 productos.\n",
      "✅ Archivo final generado: predicciones_febrero2020_completas.csv (780 productos)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Cargar predicciones obtenidas con lags ---\n",
    "df_predicciones = pd.read_csv(\"predicciones_febrero2020_lags_manual.csv\")\n",
    "\n",
    "# --- 2. Cargar lista completa de product_id a predecir ---\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    product_ids = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "df_objetivo = pd.DataFrame({\"product_id\": product_ids})\n",
    "\n",
    "# --- 3. Detectar los productos que faltan ---\n",
    "df_merged = df_objetivo.merge(df_predicciones, on=\"product_id\", how=\"left\", indicator=True)\n",
    "faltantes = df_merged[df_merged[\"_merge\"] == \"left_only\"][\"product_id\"].tolist()\n",
    "\n",
    "print(f\"❗ Faltan predicciones para {len(faltantes)} productos.\")\n",
    "\n",
    "# --- 4. Calcular el promedio de ventas de 2019 por producto ---\n",
    "df_sellin[\"periodo\"] = df_sellin[\"periodo\"].astype(str)\n",
    "df_filtrado_2019 = df_sellin[\n",
    "    (df_sellin[\"product_id\"].isin(faltantes)) &\n",
    "    (df_sellin[\"periodo\"].between(\"201901\", \"201912\"))\n",
    "]\n",
    "\n",
    "df_promedios = (\n",
    "    df_filtrado_2019.groupby(\"product_id\")[\"tn\"]\n",
    "    .sum()\n",
    "    .div(12)  # promedio mensual\n",
    "    .reset_index()\n",
    "    .rename(columns={\"tn\": \"tn\"})\n",
    ")\n",
    "\n",
    "print(f\"✅ Calculados promedios para {df_promedios.shape[0]} productos.\")\n",
    "\n",
    "# --- 5. Unir ambas fuentes de predicción ---\n",
    "df_final = pd.concat([df_predicciones, df_promedios], axis=0).sort_values(\"product_id\").reset_index(drop=True)\n",
    "\n",
    "# --- 6. Guardar resultado final ---\n",
    "df_final.to_csv(\"predicciones_febrero2020_completas.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Archivo final generado: predicciones_febrero2020_completas.csv ({df_final.shape[0]} productos)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd09565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble generado para 780 productos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Cargar predicciones finales y promedio previo ---\n",
    "df_final = pd.read_csv(\"predicciones_febrero2020_completas.csv\")  # tn por producto_id\n",
    "df_promedio_simple = pd.read_csv(\"promedio_12m_febrero2020_2.csv\")  # tn por producto_id\n",
    "\n",
    "# --- 2. Renombrar columnas para claridad ---\n",
    "df_final = df_final.rename(columns={\"tn\": \"tn_modelo\"})\n",
    "df_promedio_simple = df_promedio_simple.rename(columns={\"tn\": \"tn_promedio\"})\n",
    "\n",
    "# --- 3. Combinar ambos por product_id ---\n",
    "df_ensemble = df_final.merge(df_promedio_simple, on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Calcular promedio simple (ensemble) ---\n",
    "df_ensemble[\"tn_ensemble\"] = (\n",
    "    df_ensemble[\"tn_modelo\"] + df_ensemble[\"tn_promedio\"]\n",
    ") / 2\n",
    "\n",
    "# --- 5. Guardar resultado final ---\n",
    "df_ensemble[[\"product_id\", \"tn_ensemble\"]].to_csv(\"predicciones_febrero2020_ensemble_final.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Ensemble generado para {df_ensemble.shape[0]} productos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
